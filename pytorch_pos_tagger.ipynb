{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eff8c37",
   "metadata": {},
   "source": [
    "## Pytorch Exercise: Augmenting the LSTM part-of-speech tagger with character-level feature\n",
    "This notebook implements a POS tagger using PyTorch. The model uses both word embeddings and character-level embeddings to predict the POS tags for each word in a sentence. The notebook is buit following the instructions from the PyTorch tutorial: \"Sequence Models and Long Short-Term Memory Networks\" (https://docs.pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html).\n",
    "\n",
    "The text for training and validation is the first 4 chapters of \"Moby Dick\" by Herman Melville. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee60f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is:  True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Cuda is: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8bd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, char_hidden_dim, hidden_dim, vocab_size, num_chars, output_dim):\n",
    "        super(POSTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_embedding_dim)\n",
    "\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True)\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, char_list):\n",
    "        word_embedded = self.word_embedding(text)\n",
    "        word_embedded = word_embedded.unsqueeze(1)\n",
    "\n",
    "        word_char_embedded = []\n",
    "        for word in char_list:\n",
    "            char_embedded = self.char_embedding(word)\n",
    "            char_embedded = self.char_lstm(char_embedded.unsqueeze(0))[0][:, -1, :]\n",
    "            word_char_embedded.append(char_embedded)\n",
    "        word_char_embedded = torch.stack(word_char_embedded, dim=0)\n",
    "\n",
    "        combined = torch.cat((word_embedded, word_char_embedded), dim=2)\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        tag_space = self.fc(lstm_out.view(-1, lstm_out.shape[2]))\n",
    "        output = F.log_softmax(tag_space, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_loc = '/content/drive/MyDrive/Colab Notebooks/moby_dick_four_chapters.txt'\n",
    "\n",
    "with open(file_loc, 'r') as f:\n",
    "    whole_text = f.read()\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "doc = nlp_en(whole_text)\n",
    "all_sents = list(doc.sents)\n",
    "len(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c025d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAJOR_TAGS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'AUX', 'PROPN', 'NUM', 'X']\n",
    "\n",
    "def modify_word(word):\n",
    "    return word.lower().strip()\n",
    "\n",
    "def filter_pos_tag(pos_tag):\n",
    "    return pos_tag if pos_tag in MAJOR_TAGS else 'X'\n",
    "\n",
    "all_data = [\n",
    "    (\n",
    "        [modify_word(token.text) for token in sent if not token.is_punct and not token.is_space],\n",
    "        [filter_pos_tag(token.pos_) for token in sent if not token.is_punct and not token.is_space]\n",
    "    )\n",
    "    for sent in all_sents\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in all_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "chars = ''.join(list(word_to_ix.keys()))\n",
    "chars = list(set(chars))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "pos_to_ix = {pos: idx for idx, pos in enumerate(MAJOR_TAGS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e14e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['call', 'me', 'ishmael'], ['VERB', 'PRON', 'PROPN']),\n",
       " (tensor([0, 1, 2]),\n",
       "  [tensor([ 7, 33, 30, 30]),\n",
       "   tensor([18,  5]),\n",
       "   tensor([29, 19, 16, 18, 33,  5, 30])],\n",
       "  tensor([1, 4, 8])))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "all_data_vec = [\n",
    "    (\n",
    "        prepare_sequence(seq, word_to_ix),\n",
    "        [prepare_sequence(word, char_to_idx) for word in seq],\n",
    "        prepare_sequence(tags, pos_to_ix) \n",
    "    )\n",
    "    for seq, tags in all_data\n",
    "]\n",
    "all_data[0], all_data_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e5b0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params:  66173\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "\n",
    "WORD_EMBEDDING_DIM = 16\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "HIDDEN_DIM = 64\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_CHARS = len(char_to_idx)\n",
    "OUTPUT_DIM = len(MAJOR_TAGS)\n",
    "\n",
    "train_selection = np.random.choice(len(all_data_vec), size=int(0.8*len(all_data_vec)), replace=False).tolist()\n",
    "val_selection = [i for i in range(len(all_data_vec)) if i not in train_selection]\n",
    "training_data = [all_data_vec[i] for i in train_selection]\n",
    "validation_data = [all_data_vec[i] for i in val_selection]\n",
    "\n",
    "model = POSTagger(WORD_EMBEDDING_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, HIDDEN_DIM, VOCAB_SIZE, NUM_CHARS, OUTPUT_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "print(\"# params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca00d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniqueCountsResult(values=array([2, 5, 6, 9]), counts=array([ 212, 1013,  277,  623]))\n",
      "UniqueCountsResult(values=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), counts=array([404, 286, 150, 137, 261, 245, 267,  96,  55,  15, 209]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       404\n",
      "           1       0.00      0.00      0.00       286\n",
      "           2       0.07      0.09      0.08       150\n",
      "           3       0.00      0.00      0.00       137\n",
      "           4       0.00      0.00      0.00       261\n",
      "           5       0.03      0.11      0.04       245\n",
      "           6       0.11      0.11      0.11       267\n",
      "           7       0.00      0.00      0.00        96\n",
      "           8       0.00      0.00      0.00        55\n",
      "           9       0.02      0.67      0.03        15\n",
      "          10       0.00      0.00      0.00       209\n",
      "\n",
      "    accuracy                           0.04      2125\n",
      "   macro avg       0.02      0.09      0.02      2125\n",
      "weighted avg       0.02      0.04      0.02      2125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03811764705882353"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy calculation\n",
    "def calculate_accuracy(model, validation_data):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for text, char, y in validation_data:\n",
    "        output = model(text, char)\n",
    "        output = output.argmax(dim=1)\n",
    "        preds.append(output)\n",
    "        targets.append(y)\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).detach().cpu().numpy()\n",
    "    targets = torch.cat(targets, dim=0).detach().cpu().numpy()\n",
    "    print(classification_report(targets, preds))\n",
    "    return accuracy_score(targets, preds)\n",
    "\n",
    "# Model before training\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eccb604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 745.8057\n",
      "Epoch 20/100, Loss: 664.5018\n",
      "Epoch 30/100, Loss: 601.3794\n",
      "Epoch 40/100, Loss: 543.6922\n",
      "Epoch 50/100, Loss: 493.3460\n",
      "Epoch 60/100, Loss: 453.3121\n",
      "Epoch 70/100, Loss: 419.5597\n",
      "Epoch 80/100, Loss: 389.6323\n",
      "Epoch 90/100, Loss: 364.0924\n",
      "Epoch 100/100, Loss: 341.4937\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 100\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for sentence, char_sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence, char_sentence)\n",
    "        loss = loss_function(tag_scores, tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52fc57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniqueCountsResult(values=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), counts=array([595, 255,  49,  95, 259, 219, 269, 119,  15,   9, 241]))\n",
      "UniqueCountsResult(values=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), counts=array([404, 286, 150, 137, 261, 245, 267,  96,  55,  15, 209]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.70      0.56       404\n",
      "           1       0.48      0.43      0.45       286\n",
      "           2       0.51      0.17      0.25       150\n",
      "           3       0.44      0.31      0.36       137\n",
      "           4       0.90      0.89      0.89       261\n",
      "           5       0.92      0.82      0.87       245\n",
      "           6       0.87      0.87      0.87       267\n",
      "           7       0.57      0.71      0.63        96\n",
      "           8       0.13      0.04      0.06        55\n",
      "           9       0.89      0.53      0.67        15\n",
      "          10       0.76      0.88      0.82       209\n",
      "\n",
      "    accuracy                           0.66      2125\n",
      "   macro avg       0.63      0.58      0.58      2125\n",
      "weighted avg       0.65      0.66      0.64      2125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6583529411764706"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model after training\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1b8e4",
   "metadata": {},
   "source": [
    "#### Further things to do\n",
    "\n",
    "- Train it further to imporve accuracy.\n",
    "- Experiment with different model hyperparameters.\n",
    "- Batch the training data and vectorise steps in the forward funciton to make model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a762e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
