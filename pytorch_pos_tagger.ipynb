{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eff8c37",
   "metadata": {},
   "source": [
    "## Pytorch Exercise: Augmenting the LSTM part-of-speech tagger with character-level feature\n",
    "This notebook implements a POS tagger using PyTorch. The model uses both word embeddings and character-level embeddings to predict the POS tags for each word in a sentence. The notebook is buit following the instructions from the PyTorch tutorial: \"Sequence Models and Long Short-Term Memory Networks\" (https://docs.pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html).\n",
    "\n",
    "The text for training and validation is the first 4 chapters of \"Moby Dick\" by Herman Melville. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fb0b7",
   "metadata": {},
   "source": [
    "LEARNING OUTCOMES:\n",
    "- Understand how to implement sequence-to-sequence models with LSTMs in PyTorch.\n",
    "- Use cases of embeddings and Bidirectional LSTMs in NLP tasks.\n",
    "- Regularization and dropout techniques to prevent overfitting in deep learning models.\n",
    "- Batching and padding techniques for variable-length sequences in NLP.\n",
    "\n",
    "BEST RESULTS:\n",
    "- Training Accuracy: 0.95\n",
    "- Validation Accuracy: 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee60f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is:  True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Cuda is: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load text and process with spaCy to extract sentences\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_loc = '/content/drive/MyDrive/Colab Notebooks/moby_dick_four_chapters.txt'\n",
    "\n",
    "with open(file_loc, 'r') as f:\n",
    "    whole_text = f.read()\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "doc = nlp_en(whole_text)\n",
    "all_sents = list(doc.sents)\n",
    "len(\"Total number of senteces: \", all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant POS tags and set every other tag to X\n",
    "MAJOR_TAGS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'AUX', 'PROPN', 'NUM', 'X']\n",
    "\n",
    "def modify_word(word):\n",
    "    return word.lower().strip()\n",
    "\n",
    "def filter_pos_tag(pos_tag):\n",
    "    return pos_tag if pos_tag in MAJOR_TAGS else 'X'\n",
    "\n",
    "# Extract (words, tags) pairs from sentences, removing punctuation\n",
    "all_data = [\n",
    "    (\n",
    "        [modify_word(token.text) for token in sent if not token.is_punct and not token.is_space],\n",
    "        [filter_pos_tag(token.pos_) for token in sent if not token.is_punct and not token.is_space]\n",
    "    )\n",
    "    for sent in all_sents\n",
    "]\n",
    "\n",
    "# Build vocabularies: word → index, character → index, POS tag → index\n",
    "word_to_ix = {}\n",
    "for sent, tags in all_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "chars = ''.join(list(word_to_ix.keys()))\n",
    "chars = list(set(chars))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "pos_to_ix = {pos: idx for idx, pos in enumerate(MAJOR_TAGS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM POS Tagger with Character-level Features\n",
    "# Architecture: Word Embeddings + Character LSTM → BiLSTM → POS Tags\n",
    "class POSTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, char_hidden_dim, word_hidden_dim, vocab_size, num_chars, output_dim, dropout_rate=0.3):\n",
    "        super(POSTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_embedding_dim)\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Character LSTM: extracts character-level features for each word\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Bidirectional LSTM: processes word + character features with context from both directions\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.lstm_dropout = nn.Dropout(dropout_rate)\n",
    "        # Output layer: bidirectional LSTM outputs *2 dimensions (forward + backward)\n",
    "        self.fc = nn.Linear(word_hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, words, word_lengths, padded_chars):\n",
    "        words_embedded = self.embedding_dropout(self.word_embedding(words))\n",
    "        \n",
    "        # Get character embeddings and pack sequences for efficiency\n",
    "        # Packing skips padding tokens, preventing the LSTM from learning false patterns\n",
    "        char_embedded = self.embedding_dropout(self.char_embedding(padded_chars))\n",
    "    \n",
    "        # Get lengths for packing\n",
    "        packed_chars = nn.utils.rnn.pack_padded_sequence(\n",
    "            char_embedded, word_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "    \n",
    "        packed_out, _ = self.char_lstm(packed_chars)\n",
    "        unpacked_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # Extract last hidden state for each word\n",
    "        word_char_embedded = unpacked_out[:, -1, :]  # (num_words, char_hidden_dim)\n",
    "        \n",
    "        combined = torch.cat((words_embedded.unsqueeze(0), \n",
    "                              word_char_embedded.unsqueeze(0)), dim=2)\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        tag_space = self.fc(lstm_out.squeeze(0)) # (num_words, output_dim)\n",
    "        output = F.log_softmax(tag_space, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  2718\n",
      "Num chars:  38\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "WORD_EMBEDDING_DIM = 12\n",
    "CHAR_EMBEDDING_DIM = 6\n",
    "CHAR_HIDDEN_DIM = 6\n",
    "HIDDEN_DIM = 16\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "print(\"Vocab size: \", VOCAB_SIZE)\n",
    "NUM_CHARS = len(char_to_idx)\n",
    "print(\"Num chars: \", NUM_CHARS)\n",
    "OUTPUT_DIM = len(MAJOR_TAGS)\n",
    "DROPOUT_RATE = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e14e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['call', 'me', 'ishmael'], ['VERB', 'PRON', 'PROPN']) (tensor([0, 1, 2]), [4, 2, 7], tensor([[24, 16, 33, 33, 38, 38, 38],\n",
      "        [28, 30, 38, 38, 38, 38, 38],\n",
      "        [ 8, 34, 37, 28, 16, 30, 33]]), tensor([1, 4, 8]))\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Vectorize data: convert words/chars to indices and pad sequences\n",
    "all_data_vec = [\n",
    "    (\n",
    "        prepare_sequence(seq, word_to_ix),\n",
    "        [len(word) for word in seq],  # Character counts for packing\n",
    "        nn.utils.rnn.pad_sequence([prepare_sequence(word, char_to_idx) for word in seq], \n",
    "                                  batch_first=True,\n",
    "                                  padding_value=NUM_CHARS),  # Pad with out-of-range index\n",
    "        prepare_sequence(tags, pos_to_ix) \n",
    "    )\n",
    "    for seq, tags in all_data\n",
    "]\n",
    "print(all_data[0], all_data_vec[0])\n",
    "\n",
    "train_selection = np.random.choice(len(all_data_vec), size=int(0.8*len(all_data_vec)), replace=False).tolist()\n",
    "val_selection = [i for i in range(len(all_data_vec)) if i not in train_selection]\n",
    "training_data = [all_data_vec[i] for i in train_selection]\n",
    "validation_data = [all_data_vec[i] for i in val_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457adba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params in the model:  38157\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = POSTagger(WORD_EMBEDDING_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, HIDDEN_DIM, \n",
    "                  VOCAB_SIZE, \n",
    "                  NUM_CHARS + 1, # Add 1 for padding index\n",
    "                  OUTPUT_DIM,\n",
    "                  dropout_rate=DROPOUT_RATE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)  # L2 regularization via weight_decay\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "print(\"# params in the model: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca00d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN       0.00      0.00      0.00       477\n",
      "        VERB       0.12      0.39      0.18       300\n",
      "         ADJ       0.06      0.03      0.04       210\n",
      "         ADV       0.02      0.01      0.01       171\n",
      "        PRON       0.09      0.00      0.01       292\n",
      "         DET       0.00      0.00      0.00       299\n",
      "         ADP       0.04      0.01      0.01       309\n",
      "         AUX       0.03      0.06      0.04       127\n",
      "       PROPN       0.01      0.09      0.02        53\n",
      "         NUM       0.00      0.00      0.00        11\n",
      "           X       0.10      0.26      0.15       233\n",
      "\n",
      "    accuracy                           0.08      2482\n",
      "   macro avg       0.04      0.08      0.04      2482\n",
      "weighted avg       0.05      0.08      0.04      2482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08138597904915391"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def calculate_accuracy(model, validation_data, print_report=True):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for words, word_lengths, padded_chars, y in validation_data:\n",
    "        output = model(words, word_lengths, padded_chars)\n",
    "        output = output.argmax(dim=1)\n",
    "        preds.append(output)\n",
    "        targets.append(y)\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).detach().cpu().numpy()\n",
    "    targets = torch.cat(targets, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(targets, preds, target_names=MAJOR_TAGS))\n",
    "    return accuracy_score(targets, preds)\n",
    "\n",
    "# Baseline: model accuracy before training\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41851995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping: prevents overfitting by stopping when validation accuracy plateaus\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience  # Wait this many checks before stopping\n",
    "        self.min_delta = min_delta  # Minimum improvement threshold\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_acc):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_acc\n",
    "        elif val_acc > self.best_score + self.min_delta:\n",
    "            self.best_score = val_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 677.5242\n",
      "Training accuracy: 0.6127, Validation accuracy: 0.5967\n",
      "Epoch 2/100, Loss: 465.9638\n",
      "Training accuracy: 0.7386, Validation accuracy: 0.7172\n",
      "Epoch 3/100, Loss: 371.2629\n",
      "Training accuracy: 0.8152, Validation accuracy: 0.7736\n",
      "Epoch 4/100, Loss: 306.9255\n",
      "Training accuracy: 0.8637, Validation accuracy: 0.8155\n",
      "Epoch 5/100, Loss: 259.1399\n",
      "Training accuracy: 0.8935, Validation accuracy: 0.8247\n",
      "Epoch 6/100, Loss: 220.4558\n",
      "Training accuracy: 0.9146, Validation accuracy: 0.8509\n",
      "Epoch 7/100, Loss: 190.0280\n",
      "Training accuracy: 0.9333, Validation accuracy: 0.8634\n",
      "Epoch 8/100, Loss: 165.3925\n",
      "Training accuracy: 0.9411, Validation accuracy: 0.8618\n",
      "Epoch 9/100, Loss: 153.5479\n",
      "Training accuracy: 0.9466, Validation accuracy: 0.8662\n",
      "Epoch 10/100, Loss: 142.6477\n",
      "Training accuracy: 0.9552, Validation accuracy: 0.8747\n",
      "Epoch 11/100, Loss: 133.7159\n",
      "Training accuracy: 0.9552, Validation accuracy: 0.8687\n",
      "Epoch 12/100, Loss: 120.4329\n",
      "Training accuracy: 0.9617, Validation accuracy: 0.8703\n",
      "Epoch 13/100, Loss: 115.8698\n",
      "Training accuracy: 0.9621, Validation accuracy: 0.8735\n",
      "Epoch 14/100, Loss: 113.6380\n",
      "Training accuracy: 0.9653, Validation accuracy: 0.8715\n",
      "Epoch 15/100, Loss: 95.1406\n",
      "Training accuracy: 0.9669, Validation accuracy: 0.8723\n",
      "\n",
      "Early stopping triggered at epoch 15\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation and early stopping\n",
    "NUM_EPOCHS = 100\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()  # Enable dropout for regularization\n",
    "    total_loss = 0\n",
    "    for words, word_lengths, padded_chars, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(words, word_lengths, padded_chars)\n",
    "        loss = loss_function(tag_scores, tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Disable dropout for evaluation\n",
    "    with torch.no_grad():\n",
    "        train_acc = calculate_accuracy(model, training_data, print_report=False)\n",
    "        val_acc = calculate_accuracy(model, validation_data, print_report=False)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}\")\n",
    "    print(f\"Training accuracy: {train_acc:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    early_stopping(val_acc)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN       0.77      0.89      0.82       409\n",
      "        VERB       0.85      0.77      0.81       318\n",
      "         ADJ       0.67      0.74      0.70       190\n",
      "         ADV       0.81      0.70      0.75       156\n",
      "        PRON       0.97      0.96      0.96       308\n",
      "         DET       0.97      0.98      0.98       242\n",
      "         ADP       0.96      0.98      0.97       300\n",
      "         AUX       0.91      0.96      0.93       125\n",
      "       PROPN       0.71      0.39      0.50        62\n",
      "         NUM       1.00      0.85      0.92        20\n",
      "           X       0.95      0.91      0.93       240\n",
      "\n",
      "    accuracy                           0.87      2370\n",
      "   macro avg       0.87      0.83      0.84      2370\n",
      "weighted avg       0.87      0.87      0.87      2370\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8704641350210971"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final evaluation on validation set\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1b8e4",
   "metadata": {},
   "source": [
    "#### Further things to do\n",
    "\n",
    "- Train it further to imporve accuracy.\n",
    "- Experiment with different model architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
