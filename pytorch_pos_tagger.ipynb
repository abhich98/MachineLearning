{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eff8c37",
   "metadata": {},
   "source": [
    "## Pytorch Exercise: Augmenting the LSTM part-of-speech tagger with character-level feature\n",
    "This notebook implements a POS tagger using PyTorch. The model uses both word embeddings and character-level embeddings to predict the POS tags for each word in a sentence. The notebook is buit following the instructions from the PyTorch tutorial: \"Sequence Models and Long Short-Term Memory Networks\" (https://docs.pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html).\n",
    "\n",
    "The text for training and validation is the first 4 chapters of \"Moby Dick\" by Herman Melville. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee60f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is:  True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Cuda is: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669c675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_loc = '/content/drive/MyDrive/Colab Notebooks/moby_dick_four_chapters.txt'\n",
    "\n",
    "with open(file_loc, 'r') as f:\n",
    "    whole_text = f.read()\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "doc = nlp_en(whole_text)\n",
    "all_sents = list(doc.sents)\n",
    "len(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c025d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAJOR_TAGS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'AUX', 'PROPN', 'NUM', 'X']\n",
    "\n",
    "def modify_word(word):\n",
    "    return word.lower().strip()\n",
    "\n",
    "def filter_pos_tag(pos_tag):\n",
    "    return pos_tag if pos_tag in MAJOR_TAGS else 'X'\n",
    "\n",
    "all_data = [\n",
    "    (\n",
    "        [modify_word(token.text) for token in sent if not token.is_punct and not token.is_space],\n",
    "        [filter_pos_tag(token.pos_) for token in sent if not token.is_punct and not token.is_space]\n",
    "    )\n",
    "    for sent in all_sents\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in all_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "chars = ''.join(list(word_to_ix.keys()))\n",
    "chars = list(set(chars))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "pos_to_ix = {pos: idx for idx, pos in enumerate(MAJOR_TAGS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a8bd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, char_hidden_dim, word_hidden_dim, vocab_size, num_chars, output_dim):\n",
    "        super(POSTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_embedding_dim)\n",
    "\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True)\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(word_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, words, word_lengths, padded_chars):\n",
    "        words_embedded = self.word_embedding(words)\n",
    "        words_embedded = words_embedded.unsqueeze(1)\n",
    "\n",
    "        # Embed characters (padding_idx will produce zero embeddings)\n",
    "        char_embedded = self.char_embedding(padded_chars)  # (num_words, max_char_len, char_embedding_dim)\n",
    "    \n",
    "        # Get lengths for packing\n",
    "        packed_chars = nn.utils.rnn.pack_padded_sequence(\n",
    "            char_embedded, word_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "    \n",
    "        packed_out, _ = self.char_lstm(packed_chars)\n",
    "        unpacked_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "    \n",
    "        # Extract last hidden state for each word\n",
    "        word_char_embedded = unpacked_out[:, (-1,), :]  # (num_words, -1, char_hidden_dim)\n",
    "\n",
    "        combined = torch.cat((words_embedded, word_char_embedded), dim=2)\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        tag_space = self.fc(lstm_out.view(-1, lstm_out.shape[2]))\n",
    "        output = F.log_softmax(tag_space, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e5b0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  2718\n",
      "Num chars:  38\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "\n",
    "WORD_EMBEDDING_DIM = 8\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "print(\"Vocab size: \", VOCAB_SIZE)\n",
    "NUM_CHARS = len(char_to_idx)\n",
    "print(\"Num chars: \", NUM_CHARS)\n",
    "OUTPUT_DIM = len(MAJOR_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26e14e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['call', 'me', 'ishmael'], ['VERB', 'PRON', 'PROPN']) (tensor([0, 1, 2]), [4, 2, 7], tensor([[34, 24,  7,  7, 38, 38, 38],\n",
      "        [35,  8, 38, 38, 38, 38, 38],\n",
      "        [12, 20, 16, 35, 24,  8,  7]]), tensor([1, 4, 8]))\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "all_data_vec = [\n",
    "    (\n",
    "        prepare_sequence(seq, word_to_ix),\n",
    "        [len(word) for word in seq],  # Get lengths for packing\n",
    "        nn.utils.rnn.pad_sequence([prepare_sequence(word, char_to_idx) for word in seq], \n",
    "                                  batch_first=True,\n",
    "                                  padding_value=NUM_CHARS),  # Pad with vocab size index which is out of range for chars\n",
    "        prepare_sequence(tags, pos_to_ix) \n",
    "    )\n",
    "    for seq, tags in all_data\n",
    "]\n",
    "print(all_data[0], all_data_vec[0])\n",
    "\n",
    "train_selection = np.random.choice(len(all_data_vec), size=int(0.8*len(all_data_vec)), replace=False).tolist()\n",
    "val_selection = [i for i in range(len(all_data_vec)) if i not in train_selection]\n",
    "training_data = [all_data_vec[i] for i in train_selection]\n",
    "validation_data = [all_data_vec[i] for i in val_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "457adba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params in the model:  28080\n"
     ]
    }
   ],
   "source": [
    "model = POSTagger(WORD_EMBEDDING_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, HIDDEN_DIM, \n",
    "                  VOCAB_SIZE, \n",
    "                  NUM_CHARS + 1, # Add 1 for padding index\n",
    "                  OUTPUT_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "print(\"# params in the model: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca00d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       455\n",
      "           1       0.17      0.13      0.15       328\n",
      "           2       0.00      0.00      0.00       188\n",
      "           3       0.00      0.00      0.00       139\n",
      "           4       0.00      0.00      0.00       319\n",
      "           5       0.12      0.96      0.21       275\n",
      "           6       0.00      0.00      0.00       294\n",
      "           7       0.00      0.00      0.00       136\n",
      "           8       0.00      0.00      0.00        60\n",
      "           9       0.00      0.00      0.00        20\n",
      "          10       0.00      0.00      0.00       255\n",
      "\n",
      "    accuracy                           0.12      2469\n",
      "   macro avg       0.03      0.10      0.03      2469\n",
      "weighted avg       0.04      0.12      0.04      2469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12434183880113406"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy calculation\n",
    "def calculate_accuracy(model, validation_data, print_report=True):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for words, word_lengths, padded_chars, y in validation_data:\n",
    "        output = model(words, word_lengths, padded_chars)\n",
    "        output = output.argmax(dim=1)\n",
    "        preds.append(output)\n",
    "        targets.append(y)\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).detach().cpu().numpy()\n",
    "    targets = torch.cat(targets, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(targets, preds))\n",
    "    return accuracy_score(targets, preds)\n",
    "\n",
    "# Model before training\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eccb604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 430.6575\n",
      "Epoch 20/100, Loss: 414.8810\n",
      "Epoch 30/100, Loss: 400.4139\n",
      "Epoch 40/100, Loss: 386.9662\n",
      "Epoch 50/100, Loss: 374.4065\n",
      "Epoch 60/100, Loss: 362.7232\n",
      "Epoch 70/100, Loss: 351.8608\n",
      "Epoch 80/100, Loss: 341.7567\n",
      "Epoch 90/100, Loss: 332.3723\n",
      "Epoch 100/100, Loss: 323.6613\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 100\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for words, word_lengths, padded_chars, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(words, word_lengths, padded_chars)\n",
    "        loss = loss_function(tag_scores, tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52fc57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.67      0.52       455\n",
      "           1       0.35      0.31      0.33       328\n",
      "           2       0.42      0.05      0.09       188\n",
      "           3       0.47      0.35      0.40       139\n",
      "           4       0.94      0.86      0.90       319\n",
      "           5       0.91      0.91      0.91       275\n",
      "           6       0.86      0.85      0.86       294\n",
      "           7       0.76      0.76      0.76       136\n",
      "           8       0.19      0.08      0.11        60\n",
      "           9       1.00      0.60      0.75        20\n",
      "          10       0.74      0.87      0.80       255\n",
      "\n",
      "    accuracy                           0.64      2469\n",
      "   macro avg       0.64      0.58      0.59      2469\n",
      "weighted avg       0.64      0.64      0.62      2469\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6411502632644795"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model after training\n",
    "calculate_accuracy(model, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1b8e4",
   "metadata": {},
   "source": [
    "#### Further things to do\n",
    "\n",
    "- Train it further to imporve accuracy.\n",
    "- Experiment with different model hyperparameters.\n",
    "- Batch the training data and vectorise steps in the forward funciton to make model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a762e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
